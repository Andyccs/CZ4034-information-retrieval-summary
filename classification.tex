\chapter{Classification}
\begin{multicols*}{2}

\section{Naive Bayes Classifier}

\noindent Bayes' rule:
$$P(c|d) = \frac{P(d|c) P(c)}{P(d)}$$

\noindent Naive Bayes classifier:
$$c=\arg\!\max_{c_j \in C} P(x_1, x_2, \ldots,x_n|c_j)P(c_j)$$
$$c=\arg\!\max_{c_j \in C} P(x_1|c_j)P(x_2|c_j)\ldots P(x_n|c_j)P(c_j)$$
$$c=\arg\!\max_{c_j \in C} P(x_j) \prod_{i=1}^n P(x_i|c_j)$$

\noindent Smoothing:
$$P(x_i|c_j) = \frac{P(x_i,c_j)}{P(c_j)}$$
$$P(x_i|c_j) = \frac{P(x_i,c_j)+1}{P(c_j)+k}$$

\noindent where $k$ is the size of vocabulary

\section{K Nearest Neighbour Classification}
\noindent Algorithm:
\begin{itemize}
    \item Define N as k nearest neighbours of d
    \item Use majority vote in N to classify d
\end{itemize}
\noindent kNN has high variance and low bias (memories more than generalise)

\section{Support Vector Machine (SVM)}
\noindent Idea: the original feature space can be mapped to some higher-dimensional feature space where the training set is separable\\

\noindent Linear separability means classes can be separated by a lines or hyperplanes\\

\noindent SVM maximises  the margin around the separating hyperplane. The decision function is specified by a subset of training samples called support vectors. 

\section{Feature Selection: Chi-square}
\noindent Motivation: to reduce training time, improve generalisation, eliminate noise features, and avoids overfitting\\

\noindent The chi-square test is used to determine whether there is a significant difference between the expected frequencies and the observed frequencies in one or more categories. 

\begin{center}
\begin{tabular}{ |c|c| } 
    \hline
    $A = \#(t,c)$ & $B = \#(\sim t,c)$ \\
    \hline 
    $C = \#(t,\sim c)$ & $D = \#(\sim t,\sim c)$ \\
    \hline
\end{tabular}
\end{center}

$$\chi^2(t,c) = \frac{N\times (AD \times BC)^2}{(A+B) \times (C+D) \times (A+C) \times (B+D)}$$
$$N=A+B+C+D$$

\end{multicols*}
